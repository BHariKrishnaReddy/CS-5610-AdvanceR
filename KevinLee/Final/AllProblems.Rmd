---
title: "Final Exam"
author: "Benakana Harikrishna Reddy"
date: "4/25/2024"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r eval=TRUE}

# Set the seed
set.seed(418)

# Set the parameter values
n <- 16
alpha <- 2
beta <- 4

# Initialize the first sample
x <- 5
y <- 0

# Create vectors to store the generated samples
x_samples <- numeric(10000)
y_samples <- numeric(10000)

# Gibbs sampling
for (i in 1:10000) {
  y <- rbeta(1, x + alpha, n - x + beta)
  x <- rbinom(1, n, y)
  x_samples[i] <- x
  y_samples[i] <- y
}

# Estimating the mean of the marginal distribution f(x) of X
sample_mean_x <- mean(x_samples)
sample_mean_x

# Comparing the sample mean with the theoretical mean
theoretical_mean <- (alpha * n) / (alpha + beta)
theoretical_mean

# Printing the results
cat("Sample mean of X:", sample_mean_x, "\n")
cat("Theoretical mean of X:", theoretical_mean, "\n")

```

```{r eval=TRUE}

# Set the seed for reproducibility
set.seed(418)

# Problem 1: Load the data and draw a scatter plot
hospital_data <- read.table("/Users/harikrishnareddy/Desktop/R/KevinLee/Final/hospital.txt", header = TRUE)
plot(hospital_data$days, hospital_data$prog, xlab = "Days of Hospitalization", ylab = "Prognostic Index", main = "Scatter Plot of Hospital Data")

# Problem 2: Use Newton's method to find the least squares estimators
# Define the objective function Q
Q <- function(params, x, y) {
  b0 <- params[1]
  b1 <- params[2]
  sum((y - b0 * exp(b1 * x))^2)
}

# Define the gradient of Q
grad_Q <- function(params, x, y) {
  b0 <- params[1]
  b1 <- params[2]
  n <- length(y)
  res <- y - b0 * exp(b1 * x)
  c(sum(-2 * res * exp(b1 * x)), sum(-2 * b0 * res * x * exp(b1 * x)))
}

# Define the Hessian of Q
hess_Q <- function(params, x, y) {
  b0 <- params[1]
  b1 <- params[2]
  n <- length(y)
  res <- y - b0 * exp(b1 * x)
  H11 <- sum(-2 * exp(2 * b1 * x))
  H12 <- sum(-2 * x * exp(2 * b1 * x))
  H21 <- H12
  H22 <- sum(-2 * b0 * x^2 * exp(2 * b1 * x))
  matrix(c(H11, H12, H21, H22), nrow = 2)
}

# Newton's method
newton_method <- function(x, y, init_params, tol = 1e-6, max_iter = 100) {
  params <- init_params
  iter <- 0
  while (iter < max_iter) {
    grad <- grad_Q(params, x, y)
    hess <- hess_Q(params, x, y)
    update <- -solve(hess, grad)
    params <- params + update
    
    # Check for convergence
    if (all(!is.na(update)) && all(abs(update) <= tol)) {
      break
    }
    iter <- iter + 1
  }
  params
}


# Apply Newton's method
x <- hospital_data$days
y <- hospital_data$prog
init_params <- c(50, 0)
newton_estimates <- newton_method(x, y, init_params)
cat("Newton's method estimates:\n")
print(newton_estimates)

# Problem 3: Use optim() function
obj_func <- function(params, x, y) {
  b0 <- params[1]
  b1 <- params[2]
  sum((y - b0 * exp(b1 * x))^2)
}

optim_estimates <- optim(par = init_params, fn = obj_func, x = x, y = y)
cat("\noptim() estimates:\n")
print(optim_estimates$par)

# Problem 4: Compare with nls() output
nls_fit <- nls(prog ~ beta0 * exp(beta1 * days), data = hospital_data, start = list(beta0 = 50, beta1 = 0))
cat("\nnls() estimates:\n")
print(coef(nls_fit))

```

```{r eval=TRUE}

# Setting the seed #
set.seed(418)

# Applying the Simplex Method
library(linprog)

# The Objective function coefficients, c 
c <- c(4, 2, 9)

# The linear constraints coefficients, A and b 
A <- rbind(c(2, 1, 1), c(1, -1, 3))
b <- c(2, 3)

# Solving the linear programming problem
solution <- solveLP(c, b, A, const.dir = rep("<=", length(b)), maximum = TRUE, lpSolve = TRUE)

# Printing the solution
print(solution)

```

```{r eval=TRUE}

# in the given jobsubmission time col was not mentioned so i have edited the main jobsubmission and added "time" col


# Load the data set
jobsubmission <- read.table("/Users/harikrishnareddy/Desktop/R/KevinLee/Final/jobsubmission.txt", header = TRUE)

# Check the structure of the loaded data frame
str(jobsubmission)

# Check the first few rows of the data frame
head(jobsubmission)

# Calculate the MLE of λ
n <- length(jobsubmission$time)
cat("Value of n:", n, "\n")
cat("Sum of times:", sum(jobsubmission$time), "\n")
mle_lambda <- n / sum(jobsubmission$time)

cat("n:", n, "mle_lambda:", mle_lambda, "\n")



# Function to calculate the MLE of λ from a sample
mle_lambda_func <- function(data) {
  n <- length(data)
  return(n / sum(data))
}

# Bootstrap function
bootstrap_se <- function(data, func, B) {
  n <- length(data)
  bootstrap_estimates <- numeric(B)
  
  for (i in 1:B) {
    bootstrap_sample <- sample(data, n, replace = TRUE)
    bootstrap_estimates[i] <- func(bootstrap_sample)
  }
  
  return(sd(bootstrap_estimates))
}

# Estimate the standard error using bootstrap
set.seed(418)  # Set the seed for reproducibility
bootstrap_se_lambda <- bootstrap_se(jobsubmission$time, mle_lambda_func, 1000)
cat("Standard error of the MLE of λ (using bootstrap):", bootstrap_se_lambda, "\n")

# Calculate the bootstrap MLE of λ for each bootstrap sample
bootstrap_mle_lambda <- numeric(1000)
for (i in 1:1000) {
  bootstrap_sample <- sample(jobsubmission$time, n, replace = TRUE)
  bootstrap_mle_lambda[i] <- mle_lambda_func(bootstrap_sample)
}

# Calculate the percentile bootstrap confidence interval
ci_lower <- quantile(bootstrap_mle_lambda, probs = 0.025, na.rm = TRUE)
ci_upper <- quantile(bootstrap_mle_lambda, probs = 0.975, na.rm = TRUE)

cat("95% Confidence Interval for the MLE of λ (percentile bootstrap):\n")
cat("Lower bound:", ci_lower, "\n")
cat("Upper bound:", ci_upper, "\n")


```



I certify here that the work on this exam is solely mine. I did not receive any assistance from others, and I did not provide any assistance to others.

PRINT YOUR NAME: Benakana Harikrishna Reddy          DATE: 04/25/24
